\documentclass{article}
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry} %Anpassung der Seitenränder
%\pagestyle{headings}
\begin{document}
	\begin{center}\huge Formelsammlung GNT\par\bigskip\large\today\end{center}%Dann eben nicht als Titel sondern manuell
	\section{Ergebnismenge}
	\begin{equation} \textstyle
	\Omega = \{ \omega : \omega_{1}, \omega_{2}, ..., \omega_{K} \}
	\end{equation}
	\section{Zufallsvariablen}
	\textbf{Zufallsvariablen}
	\begin{equation} \textstyle
	\Omega \overset{X}{\rightarrow}\mathbb{R}
	\end{equation}
	\begin{equation} \textstyle
	\Omega = \{ X(\omega) : x_{1}, x_{2}, ..., x_{k} \}
	\end{equation}
	\section{Wahrscheinlichkeit}
	\textbf{Klassicher Wahrscheinlichkeitsbegriff}
	\begin{equation} \textstyle
	P(A) ) = \frac{g}{N} = \frac{\text{Anzahl für das Eintreffen von A günstigen Fällen}}{\text{Anzahl aller gleichmöglichen Fälle}}
	\end{equation}
	\textbf{Versuchsreihen (relative Häufigkeit)}
	\begin{equation} \textstyle
		\lim_{M \rightarrow \infty} \frac{m(A)}{M} = \lim_{M \rightarrow \infty} h(A) = P(A)
	\end{equation}
	\textbf{Kolmogoroff-Axiome}
	\begin{equation} \textstyle
		P(A) :\geq 0
	\end{equation}
	\begin{equation} \textstyle
		P(\Omega) := 1
	\end{equation}
	\begin{equation} \textstyle
		P(A \cup B) := P(A) + P(B), \text{wenn } A \cap B = \emptyset
	\end{equation}
	\begin{equation} \textstyle
		A \overset{P}{\rightarrow}[0,1]
	\end{equation}
	\begin{equation} \textstyle
		A \overset{X}{\rightarrow}\mathbb{R}\overset{P}{\rightarrow}[0,1]
	\end{equation}
	\begin{equation} \textstyle
		P_{k} = P(X = x_{k})
	\end{equation}
	\begin{equation} \textstyle
		P(A) + P(\overline{A}) = 1
	\end{equation}
	\begin{equation} \textstyle
		P(\emptyset) = 0
	\end{equation}
	\begin{equation} \textstyle
		P(B) \geq P(A), \text{wenn } A \subseteq B
	\end{equation}
	\begin{equation} \textstyle
		0 \leq P(A) \leq 1
	\end{equation}
	\begin{equation} \textstyle
		P(A \cup B) = P(A) + P(B) - P(A \cap B)
	\end{equation}
	\textbf{Laplace-Experiment}
	\begin{equation} \textstyle
		P(\omega_{k}) = \frac{1}{K}; \forall k
	\end{equation}
	\textbf{Bedingte Wahrscheinlichkeit}
	\begin{equation} \textstyle
		P(B|A) = \frac{P(A \cap B)}{P(A)}; P(A)  > 0
	\end{equation}
	\begin{equation} \textstyle
		P(A|B) = \frac{P(A \cap B)}{P(B)}; P(B)  > 0
	\end{equation}
	\begin{equation} \textstyle
		P(B|A) \geq 0
	\end{equation}
	\begin{equation} \textstyle
		P(B \cup C | A) = P(B|A) + P(C|A), \text{wenn } B \cap C = \emptyset
	\end{equation}
	\textbf{Bayes}
	\begin{equation} \textstyle
		P(B|A) = \frac{P(B)}{P(A)} P(A|B)
	\end{equation}
	\begin{equation} \textstyle
		P(A|B) = \frac{P(A)}{P(B)} P(B|A)
	\end{equation}
	\begin{equation} \textstyle
		P(B) = \sum_{k=1}^{N} P(A_{k}) \cdot P(B|A_{k})
	\end{equation}
	\textbf{Statische Unabhängigkeit}
	\begin{equation} \textstyle
		P(B|A) := P(B)
	\end{equation}
	\begin{equation} \textstyle
		P(A \cap B) = P(A) \cdot P(B)
	\end{equation}
	\subsection{Verteilungsfuktions, Verteilungsdichtefunktion}
	\textbf{Verteilungsfunktion}
	\begin{equation} \textstyle
		F_{X}(x) := P(\omega : X(\omega) \leq x) = P(X \leq x)
	\end{equation}
	\begin{equation} \textstyle
		F_{X}(x) \geq 0
	\end{equation}
	\begin{equation} \textstyle
		P(\omega : X(\omega) > x) = 1 - F_{X}(x)
	\end{equation}
	\begin{equation} \textstyle
		P(- \infty) = 0;\hspace{0.5cm} F_{X}(\infty) = 1
	\end{equation}
	\begin{equation} \textstyle
		F_{X}(b) - F_{X}(a) = P(a < X \leq b)
	\end{equation}
	\textbf{Verteilungsdichtefunktion}
	\begin{equation} \textstyle
	p_{X}(x) = \frac{dF_{X}(x)}{dx}
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \frac{dF_{X}(x)}{dx}
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b) = F_{X}(b) - F_{X}(a) = \int_{a}^{b} p_{X}(x)dx
	\end{equation}
	\begin{equation} \textstyle
		P(- \infty < X \leq \infty) = \int_{- \infty}^{\infty} p_{X}(x)dx =1
	\end{equation}
	\begin{equation} \textstyle
		\int_{a}^{a} p_{X}(x) dx = P\{X = a\} = 0
	\end{equation}
	\textbf{Normalverteilung $N(\mu_{X},\sigma_{X})$}
	\begin{equation} \textstyle
		F_{X}(x) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{x - \mu_{X}}{\sigma_{X} \sqrt{2}} \right) \right]
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \frac{1}{\sigma_{X} \sqrt{2 \pi}}  \exp  \left[ \frac{-(x - \mu_{X})^{2}}{2\sigma_{X}^{2}}\right]
	\end{equation}
	\textbf{Fehlerfunktion}
	\begin{equation} \textstyle
		\operatorname{erf}(x) = \frac 2{\sqrt\pi} \int_0^x e^{-t^2}\,\mathrm dt
	\end{equation}
	\subsection{Verbundverteilungen}
	\begin{equation} \textstyle
		F_{XY}(x,y) := P(X \leq x, y \leq Y)
	\end{equation}
	\begin{equation} \textstyle
		p_{XY}(x,y) := \frac{\delta^{2} F_{XY}(x,y)}{\delta x \delta y}
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b, c < Y \leq d) = \int_{a}^{b} \int_{c}^{d} p_{XY}(x,y)dydx
	\end{equation}
	\begin{equation} \textstyle
		\int_{- \infty}^{\infty} \int_{- \infty}^{\infty} p_{XY}(x,y)dydx = 1
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b, c < Y \leq d) = F_{XY}(b,d) -  F_{XY}(a,d) -  F_{XY}(b,c) +  F_{XY}(a,c)
	\end{equation}
	\subsection{Randdichteverteilung}
	\begin{equation} \textstyle
		p_{X}(x) = \int_{- \infty}^{\infty} p_{XY}(x,y) dy
	\end{equation}
	\begin{equation} \textstyle
		p_{Y}(y) = \int_{- \infty}^{\infty} p_{XY}(x,y) dx
	\end{equation}
	\subsection{Bedingte Verteilungsdichtefunktion}
	\begin{equation} \textstyle
		p_{X}(x|Y=y) := \frac{p_{XY}(x,y)}{p_{Y}(y)}
	\end{equation}
	\begin{equation} \textstyle
		p_{Y}(y|X=x) := \frac{p_{XY}(x,y)}{p_{X}(x)}
	\end{equation}
	\textbf{Statistische Unabhängigkeit}
	\begin{equation} \textstyle
		p_{XY}(x,y) = p_{X}(x) \cdot p_{Y}(y)
	\end{equation}
	\subsection{Diskete Zufallsvariablen}
	\begin{equation} \textstyle
		F_{X}(x) = \sum_{k=1}^{N} P_{k}\sigma(x-x_{k}), \hspace{0.5cm}\text{wobei } \sigma(x)  \text{ Sprungfuntion}
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \frac{dF_{X}(x)}{dx}
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \sum_{k=1}^{N} P_{k}\delta(x-x_{k})
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b) = F_{X}(b) - F_{X}(a) = \int_{a}^{b} p_{X}(x) dx
	\end{equation}
	\textbf{Bernoulli-Experiment}
	\begin{equation} \textstyle
		P_{i} = P(\text{genau i-faches Auftreten}) = B(n,p,i) = \binom{n}{i} \cdot p^{i} \cdot (1-p)^{n-i}
	\end{equation}
	Bei kleinen p-Warten und großen N kann Poission-Verteilung verwendet werden
	\begin{equation} \textstyle
		P_{i} \approx \frac{(np)^{i}}{i!} e^{-np}
	\end{equation}
	\begin{equation} \textstyle
		P(\text{bis zu i-faches Auftreten}) = \sum_{j=0}^{i} B(n,p,j)
	\end{equation}
	\begin{equation} \textstyle
		P(\text{mehr als i-faches Auftreten}) = 1 - \sum_{j=0}^{i} B(n,p,j)
	\end{equation}
	\begin{equation} \textstyle
		\mu_{X} = np
	\end{equation}
	\begin{equation} \textstyle
		\sigma_{x}^{2} = np(1-p)
	\end{equation}
	\begin{equation} \textstyle
		P(A) = \sum_{k=1}^{N} P(A|B_{k}) P(B_{k}) 
	\end{equation}
	\textbf{FRAGE IM FORUM}
	\begin{equation} \textstyle
		P(A) = \sum_{k=1}^{N} P(A,B=B_{k})
	\end{equation}
	\subsection{Transformation}
	\begin{equation} \textstyle
		p_{Y}(y) = \frac{p_{X}(x)}{|g'(x)|}\Big|_{x=g^{-1}(y)}
	\end{equation}
	\begin{equation} \textstyle
		P(Y \leq y_{0}) = P(X \leq x_{0})
	\end{equation}
	\begin{equation} \textstyle
		\int_{- \infty}^{y_{0}} p_{y}(y) dy = \int_{- \infty}^{x_{0}=g^{-1}(y_{0})} p_{x}(x) dx
	\end{equation}
	\begin{equation} \textstyle
		p_{Y} = p_{X} \frac{d x_{0}}{d y_{0}} = \frac{p_{X}}{\frac{d y_{0}}{d x_{0}}}
	\end{equation}
	\textbf{Lineare Transformation $Y = aX + b$}
	\begin{equation} \textstyle
			p_{Y} = p_{X}\left( \frac{y-b}{a} \right) \frac{1}{|a|}
	\end{equation}
	\subsection{Summe von Zufallsvariablen}
	\textbf{$Z = X+Y$, wobei X und Y statistisch Unabhängig sein müssen:}
	\begin{equation} \textstyle
		p_{Z} = \int_{- \infty}^{\infty} p_{X}(\alpha)p_{Y}(z-\alpha) d \alpha = p_{X}(z) \ast p_{Y}(z)
	\end{equation}
	\textbf{Beweis}
	Aus $Z = X+Y$ wird $Y = Z-X$, und damit ist das Ereignis $(Z \leq a)$ identisch mit dem Verbundereignis ($Y \leq a - X$ und $- \infty \leq X \leq \infty )$. Für die Verteilungsfunktion $P(Z \leq a)$ gilt:
	\begin{equation} \textstyle
	P(Z \leq a) = P(X \leq \infty, Y \leq a -X) = F_{Z}(a) = \int_{x = - \infty}^{\infty} p_{XY}(x,a-x) dx
	\end{equation}
	Für die VDF folgt
	\begin{equation} \textstyle
	p_{z}(a) = \frac{F_{z}(a)}{da} = \int_{- \infty}^{\infty} p_{XY}(x,a-x) dx
	\end{equation}
	, die bei statistischer Unabhängigkeit zu
	\begin{equation} \textstyle
	p_{z}(a) = \int_{X} p_{X}(x)p_{Y}(a-x)dx = p_{X}(a) \ast p_{Y}(a)
	\end{equation}
	\subsection{Erwartungswerte}
	Der Erwartungswert E[(x)] einer transformierten Zufallsvariablen Y = g(X) ist definiert als
	\begin{equation} \textstyle
		E[g(x)] = \int_{- \infty}^{\infty} g(x) p_{X}(x) dx
	\end{equation}
	\textbf{Mittelwert}
	\begin{equation} \textstyle
		\mu_{x} = E[X] = \int_{- \infty}^{\infty} x p_{X}(x) dx
	\end{equation}
	\textbf{Quadratischer Mittelwert (Leistung)}
	\begin{equation} \textstyle
		P_{X} = E[X^{2}] = \int_{- \infty}^{\infty} x^{2} p_{X}(x) dx 
	\end{equation}
	\textbf{Varianz}
	\begin{equation} \textstyle
		\sigma_{X}^{2} = E[(X-\mu_{X})^{2}] = \int_{- \infty}^{\infty} (x-\mu_{X})^{2} p_{X}(x) dx
	\end{equation}
	Aus dieser Gleichung folgt auch
	\begin{equation} \textstyle
		\sigma_{X}^{2} = P_{X} - \mu_{X}^{2}
	\end{equation}
	\subsection{Erwartungswerte für wertdiskrete Zufallsvariablen}
	\begin{equation} \textstyle
		p_{X} = \sum_{k=1}^{N} P_{k} \delta(x-x_{k})
	\end{equation}
	\textbf{Mittelwert}
	\begin{equation} \textstyle
		\mu_{X} = E[X] = \sum_{k=1}^{N} P_{k} x_{k}
	\end{equation}
	\textbf{Quadratischer Mittelwert (Leistung)}
	\begin{equation} \textstyle
		\mu_{X} = E[X^{2}] = \sum_{k=1}^{N} P_{k} x_{k}^{2}
	\end{equation}
	\textbf{Varianz}
	\begin{equation} \textstyle
		\sigma_{X}^{2} = E[(X-\mu_{X})^{2}] = \sum_{k=1}^{N} P_{k} (x_{k}-\mu_{X})^{2}
	\end{equation}
	\subsection{Schätsgrößen für Erwartungswerte}
	\begin{equation} \textstyle
		m_{X} = \frac{1}{N} \sum_{n = 0}^{N - 1}x(n)
	\end{equation}
	\begin{equation} \textstyle
		E[M_{X}] = \frac{1}{M} \sum_{j = 1}^{M} \mu_{X} = \frac{1}{M}M \cdot \mu_{X} = \mu_{X}
	\end{equation}
	\begin{equation} \textstyle
		\lim_{N \rightarrow \infty} E[(M_{X} - \mu_{X})^{2}] = 0
	\end{equation}
	\begin{equation} \textstyle
		\sigma^{2}(m_{X}) = \frac{\sigma_{X}^{2}}{M}
	\end{equation}
	\textbf{Varianz}
	\begin{equation} \textstyle
		s_{X}^{2} = \frac{1}{N - 1} \sum_{n = 0}^{N - 1} [x(n) - m_{X}]^{2}
	\end{equation}
	\begin{equation} \textstyle
		E[S_{X}^{2}] = \sigma_{X}^{2} \hspace{0.5cm} ; \hspace{0.5cm} \sigma(S_{X}^{2}) = \frac{2\sigma_{X}^{2}}{N - 1}
	\end{equation}
	\subsection{Schätsgrößen für Erwartungswerte}
	\begin{equation} \textstyle
	E[Z] = \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} g(x,y) p_{XY}(x,y) dx dy
	\end{equation}
	Mit $Z = X+Y$, $Z = (X+Y)^{2}$, $Z = XY$, ergeben sich die folgenden Ergebnisse:
	\textbf{Addition von Zufallsvariablen}
	\begin{equation} \textstyle
		E[X+Y] = E[X] + E[Y]
	\end{equation}
	\textbf{Multiplikation von Zufallsvariablen}
	\begin{equation} \textstyle
		E[XY] = E[X] \cdot E[Y], \hspace{0.5cm} \text{ bei statist. Unabhängigkeit}
	\end{equation}
	\textbf{Multiplikation von Zufallsvariablen}
	\begin{equation} \textstyle
		E[(X+Y)^{2}] = P_{Z} = P_{X} + P_{Y} + 2 E[XY]
	\end{equation}
	\begin{equation} \textstyle
		\sigma_{Z}^{2} = \sigma_{X}^{2} + \sigma_{Y}^{2} \text{ bei statis. Unabhängigkeit}
	\end{equation}
	\textbf{Kovarianz}
	\begin{equation} \textstyle
		C_{XY} = E[(X - \mu_{X}) (Y - \mu_{Y})]
	\end{equation}
	\textbf{Kreuzkorrelationskoeffizient}
	\begin{equation} \textstyle
		p_{XY} = \frac{C_{XY}}{\sigma_{X} \sigma_{Y}}
	\end{equation}
	\textbf{Keine Korrelation}
	\begin{equation} \textstyle
		C_{XY} = 0
	\end{equation}
	\textbf{Orthogonak zueinander}
	\begin{equation} \textstyle
		E[XY] = 0
	\end{equation}
	Unkorreliertheit ist eine sehr viel schwächere Forderung als statistische Unabhängigkeit, da bei der letztern
	\begin{equation} \textstyle
		p_{XY}(x,y) = p_{X}(x) \cdot p_{Y}(y)
	\end{equation}
	f+r jedes $x$ und $y$ gelten muss, während $p_{XY}$ bei Unkorreliertheit unter einem Integral steht.
	\section{Stochastische Prozesse}
	\subsection{Eigenschaften}
	\begin{itemize}
		\item Stationär: TODO
		\item Ergodizität: TODO
	\end{itemize}
	\subsection{Ordnungen}
	\textbf{Verteilung erster Ordnung:} Für einen beliebigen Zeitpunkt $i$ und einen vorgegebenen Zahlenwert $\alpha$ beschreibt die Verteilungsfunktion die Verteilung der Amplitudenwerte im Ensemble:
	\begin{equation} \textstyle
		F_{X}(\alpha;i) = P(X(i) \leq \alpha)
	\end{equation}
	Für unterschiedlichste Zeitpunkte $i$ können die Verteilungsfunktion verschieden sein.\\
	\textbf{Verteilung zweiter Ordnung:} Für zwei beliebige Zeitpunkte $i$ und $j$ und zwei vorgegebene Zahlenwerten $\alpha$ und $\beta$ beschreibt die Verbundverteilungsfunktion den Zusammenhang zwischen den zu diesen Zeitpunkten auftretenden Amplitudenwerten:
	\begin{equation} \textstyle
	F_{X}(\alpha,\beta;i,j) = P(X(i) \leq \alpha; X(j) \leq \beta)
	\end{equation}
	\textbf{Verteilung dritter Ordnung:} Für drei beliebige Zeitpunkte $i$, $j$ und $k$ und drei vorgegebene Zahlenwerten $\alpha$, $\beta$ und $\gamma$ beschreibt die Verbundverteilungsfunktion den Zusammenhang zwischen den zu diesen Zeitpunkten auftretenden Amplitudenwerten:
	\begin{equation} \textstyle
	F_{X}(\alpha,\beta,\gamma;i,j,k) = P(X(i) \leq \alpha; X(j) \leq \beta; X(k) \leq \gamma)
	\end{equation}
	\textbf{Verallgemeinerung:} Ist die Beschreibung für die Verteilungsfunktionen beliebiger Ordnung bekannt, so ist der stochastische Prozess $\{ X(n) \}$ eindeutig beschrieben. Anstatt der Verbundverteilungsfunktionen können auch die Verbundverteilungsdichtefunktionen
	\begin{equation} \textstyle
		p_{X}(\alpha,\beta,\gamma,...;i,j,k,...)
	\end{equation}
	verwendet werden. Durch sie können die Wahrscheinlichkeiten
	\begin{equation} \textstyle
		p_{X}(\alpha,\beta,\gamma,...;i,j,k,...) d\alpha d\beta d\gamma
	\end{equation}
	angegeben werden, also die Wahrscheinlichkeiten, dass die Musterfolgen im Zeitpunkt $i$ Werte zwischen $\alpha$ und $\alpha + \delta \alpha$, im Zeitpunkt $j$ Werte zwischen $\beta$ und $\beta + \delta \beta$, im Zeitpunkt $k$ Werte zwischen $\gamma$ und $\gamma + \delta \gamma$ annehmen, etc.
	\subsection{Erwartungswerte eines stochatischen Prozesses}
	\textbf{Arithmetischer Mittelwert}
	\begin{equation} \textstyle
		\mu_{X}(n) := E[X(n)]
	\end{equation}
	Ist der Prozess stationär, so ist der Mittelwert unabhängig von dem absoluten Zeitpunkt n:
	\begin{equation} \textstyle
		\mu_{X} := E[X(n)]
	\end{equation}
	\textbf{Autokorrelationsfolge}
	\begin{equation} \textstyle
		R_{XX}(i,j) := E[X(i)X(j)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{X}(\alpha,\beta;i,j)d\alpha d\beta
	\end{equation}
	Ist der Mittlwert nur von der Zeitdifferent $k = j - i$ abhängig (also nicht von der absoluten Zeit $i$), so
	\begin{equation} \textstyle
		R_{XX}(n,n+k) := R_{XX}(k) := E[X(n)X(n+k)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{X}(\alpha,\beta;k)d\alpha d\beta
	\end{equation}
	\textbf{Autokovarianzfolge}
	\begin{equation} \textstyle
		C_{XX}(k) = E[(X(n) - \mu_{X}) ( X(n+k) - \mu_{X} )]
	\end{equation}
	Beschreibt die AKF des vom arithmetischen Mittelwert befreiten stochastischen Prozess $\{ X(n) \}$. Für mittelwertfreie Prozesse sind $R_{XX}(k)$ und $C_{XX}(k)$ identisch.\\
	\textbf{AKF Eigenschaften}
	\begin{equation} \textstyle
		R_{XX}(k) = R_{XX}(-k)
	\end{equation}
	\begin{equation} \textstyle
		R_{XX}(0) = P_{X}
	\end{equation}
	\begin{equation} \textstyle
		- R_{XX}(0) \leq  R_{XX}(k) \leq R_{XX}(0)
	\end{equation}
	Ist der Prozess nicht mittelwertfrei, so ist
	\begin{equation} \textstyle
		R_{XX}(0) = P_{X} = \sigma_{X}^{2} + \mu_{X}^{2}
	\end{equation}
	\begin{equation} \textstyle
		\lim R_{XX}(k) = 0
	\end{equation}
	\begin{equation} \textstyle
		p_{XX}(k) = \frac{R_{XX}(k)}{R_{XX}(0)}
	\end{equation}
	\begin{equation} \textstyle
	-1 \leq p_{XX}(k) \leq 1
	\end{equation}
	Kurzschreibweise
	\begin{equation} \textstyle
		p_{XX}(k) = p(k)
	\end{equation}
	\textbf{Autokorrelationsmatrix $R_{XX}$}
	\begin{equation} \textstyle
		\mathbf{R_{XX}} =
		 \begin{bmatrix}
		R_{XX}(0) & R_{XX}(1) & R_{XX}(2) & ... & R_{XX}(N-1)\\
		R_{XX}(1) & R_{XX}(0) & R_{XX}(1) & ... & R_{XX}(N-2)\\
		.. & ... & ... & ... & ...\\
		R_{XX}(N-1) & R_{XX}(N-2) & R_{XX}(1) & ... & R_{XX}(0)\\
		\end{bmatrix}
	\end{equation}
	\begin{equation} \textstyle
		|R_{XX}(k)| \geq 0
	\end{equation}
	\subsection{Kreuzkorrelationsfolgen}
	\begin{equation} \textstyle
		R_{XY}(i,j) := E[X(i)Y(j)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{XY}(\alpha,\beta;i,j)d\alpha d\beta
	\end{equation}
	Nur von der Zeitdifferenz abhängig...
	\begin{equation} \textstyle
		R_{XY}(n,n+k) := R_{XX}(k) := E[X(n)Y(n+k)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{XY}(\alpha,\beta;k)d\alpha d\beta
	\end{equation}
	\textbf{Eigenschaften KKF}
	\begin{equation} \textstyle
		R_{XY}(k) = R_{YX}(-k)
	\end{equation}
	\begin{equation} \textstyle
		R_{XY}^{2}(k) \leq R_{XX}(0) R_{YY})(0)
	\end{equation}
	\textbf{Unkorreliert}
	\begin{equation} \textstyle
		R_{XY}(k) = \mu_{X} \cdot \mu_{Y}, \hspace{0.5cm} \forall k
	\end{equation}
	\textbf{Statistische Unabhängigkeit}
	\begin{equation} \textstyle
		p_{XY}(\alpha,\beta;k) = p_{X}(\alpha;k)  \cdot p_{Y}(\beta;k)
	\end{equation}
	\textbf{Orthogonalität}
	\begin{equation} \textstyle
		R_{YX}(k) := 0; \hspace{0.5cm} \forall k
	\end{equation}
	\textbf{Addition von Zufallsvariable $Z(n) = X(n) + Y(n)$}
	\begin{equation} \textstyle
		R_{ZZ}(k) = R_{XX}(k) + R_{YY}(k) + R_{XY}(k) + R_{YX}(k)
	\end{equation}
	Wenn sie \textbf{nicht} miteinander korreliert sind
	\begin{equation} \textstyle
		R_{ZZ}(k) = R_{XX}(k) + R_{YY}(k)
	\end{equation}
\end{document}