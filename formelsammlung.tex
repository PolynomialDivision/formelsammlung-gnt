\documentclass{article}
\usepackage[ngerman]{babel}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage[left=3cm,right=3cm,top=2cm,bottom=2cm]{geometry} %Anpassung der Seitenränder
%\pagestyle{headings}
\begin{document}
	\begin{center}\huge Formelsammlung GNT\par\bigskip\large\today\end{center}%Dann eben nicht als Titel sondern manuell
	\section{Ergebnismenge}
	\begin{equation} \textstyle
	\Omega = \{ \omega : \omega_{1}, \omega_{2}, ..., \omega_{K} \}
	\end{equation}
	\section{Zufallsvariablen}
	\textbf{Zufallsvariablen}
	\begin{equation} \textstyle
	\Omega \overset{X}{\rightarrow}\mathbb{R}
	\end{equation}
	\begin{equation} \textstyle
	\Omega = \{ X(\omega) : x_{1}, x_{2}, ..., x_{k} \}
	\end{equation}
	\section{Wahrscheinlichkeit}
	\textbf{Klassicher Wahrscheinlichkeitsbegriff}
	\begin{equation} \textstyle
	P(A) ) = \frac{g}{N} = \frac{\text{Anzahl für das Eintreffen von A günstigen Fällen}}{\text{Anzahl aller gleichmöglichen Fälle}}
	\end{equation}
	\textbf{Versuchsreihen (relative Häufigkeit)}
	\begin{equation} \textstyle
		\lim_{M \rightarrow \infty} \frac{m(A)}{M} = \lim_{M \rightarrow \infty} h(A) = P(A)
	\end{equation}
	\textbf{Kolmogoroff-Axiome}
	\begin{equation} \textstyle
		P(A) :\geq 0
	\end{equation}
	\begin{equation} \textstyle
		P(\Omega) := 1
	\end{equation}
	\begin{equation} \textstyle
		P(A \cup B) := P(A) + P(B), \text{wenn } A \cap B = \emptyset
	\end{equation}
	\begin{equation} \textstyle
		A \overset{P}{\rightarrow}[0,1]
	\end{equation}
	\begin{equation} \textstyle
		A \overset{X}{\rightarrow}\mathbb{R}\overset{P}{\rightarrow}[0,1]
	\end{equation}
	\begin{equation} \textstyle
		P_{k} = P(X = x_{k})
	\end{equation}
	\begin{equation} \textstyle
		P(A) + P(\overline{A}) = 1
	\end{equation}
	\begin{equation} \textstyle
		P(\emptyset) = 0
	\end{equation}
	\begin{equation} \textstyle
		P(B) \geq P(A), \text{wenn } A \subseteq B
	\end{equation}
	\begin{equation} \textstyle
		0 \leq P(A) \leq 1
	\end{equation}
	\begin{equation} \textstyle
		P(A \cup B) = P(A) + P(B) - P(A \cap B)
	\end{equation}
	\textbf{Laplace-Experiment}
	\begin{equation} \textstyle
		P(\omega_{k}) = \frac{1}{K}; \forall k
	\end{equation}
	\textbf{Bedingte Wahrscheinlichkeit}
	\begin{equation} \textstyle
		P(B|A) = \frac{P(A \cap B)}{P(A)}; P(A)  > 0
	\end{equation}
	\begin{equation} \textstyle
		P(A|B) = \frac{P(A \cap B)}{P(B)}; P(B)  > 0
	\end{equation}
	\begin{equation} \textstyle
		P(B|A) \geq 0
	\end{equation}
	\begin{equation} \textstyle
		P(B \cup C | A) = P(B|A) + P(C|A), \text{wenn } B \cap C = \emptyset
	\end{equation}
	\textbf{Bayes}
	\begin{equation} \textstyle
		P(B|A) = \frac{P(B)}{P(A)} P(A|B)
	\end{equation}
	\begin{equation} \textstyle
		P(A|B) = \frac{P(A)}{P(B)} P(B|A)
	\end{equation}
	\begin{equation} \textstyle
		P(B) = \sum_{k=1}^{N} P(A_{k}) \cdot P(B|A_{k})
	\end{equation}
	\textbf{Statische Unabhängigkeit}
	\begin{equation} \textstyle
		P(B|A) := P(B)
	\end{equation}
	\begin{equation} \textstyle
		P(A \cap B) = P(A) \cdot P(B)
	\end{equation}
	\subsection{Verteilungsfuktions, Verteilungsdichtefunktion}
	\textbf{Verteilungsfunktion}
	\begin{equation} \textstyle
		F_{X}(x) := P(\omega : X(\omega) \leq x) = P(X \leq x)
	\end{equation}
	\begin{equation} \textstyle
		F_{X}(x) \geq 0
	\end{equation}
	\begin{equation} \textstyle
		P(\omega : X(\omega) > x) = 1 - F_{X}(x)
	\end{equation}
	\begin{equation} \textstyle
		P(- \infty) = 0;\hspace{0.5cm} F_{X}(\infty) = 1
	\end{equation}
	\begin{equation} \textstyle
		F_{X}(b) - F_{X}(a) = P(a < X \leq b)
	\end{equation}
	\textbf{Verteilungsdichtefunktion}
	\begin{equation} \textstyle
	p_{X}(x) = \frac{dF_{X}(x)}{dx}
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \frac{dF_{X}(x)}{dx}
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b) = F_{X}(b) - F_{X}(a) = \int_{a}^{b} p_{X}(x)dx
	\end{equation}
	\begin{equation} \textstyle
		P(- \infty < X \leq \infty) = \int_{- \infty}^{\infty} p_{X}(x)dx =1
	\end{equation}
	\begin{equation} \textstyle
		\int_{a}^{a} p_{X}(x) dx = P\{X = a\} = 0
	\end{equation}
	\textbf{Normalverteilung $N(\mu_{X},\sigma_{X})$}
	\begin{equation} \textstyle
		F_{X}(x) = \frac{1}{2} \left[ 1 + \text{erf} \left( \frac{x - \mu_{X}}{\sigma_{X} \sqrt{2}} \right) \right]
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \frac{1}{\sigma_{X} \sqrt{2 \pi}}  \exp  \left[ \frac{-(x - \mu_{X})^{2}}{2\sigma_{X}^{2}}\right]
	\end{equation}
	\textbf{Fehlerfunktion}
	\begin{equation} \textstyle
		\operatorname{erf}(x) = \frac 2{\sqrt\pi} \int_0^x e^{-t^2}\,\mathrm dt
	\end{equation}
	\subsection{Verbundverteilungen}
	\begin{equation} \textstyle
		F_{XY}(x,y) := P(X \leq x, y \leq Y)
	\end{equation}
	\begin{equation} \textstyle
		p_{XY}(x,y) := \frac{\delta^{2} F_{XY}(x,y)}{\delta x \delta y}
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b, c < Y \leq d) = \int_{a}^{b} \int_{c}^{d} p_{XY}(x,y)dydx
	\end{equation}
	\begin{equation} \textstyle
		\int_{- \infty}^{\infty} \int_{- \infty}^{\infty} p_{XY}(x,y)dydx = 1
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b, c < Y \leq d) = F_{XY}(b,d) -  F_{XY}(a,d) -  F_{XY}(b,c) +  F_{XY}(a,c)
	\end{equation}
	\subsection{Randdichteverteilung}
	\begin{equation} \textstyle
		p_{X}(x) = \int_{- \infty}^{\infty} p_{XY}(x,y) dy
	\end{equation}
	\begin{equation} \textstyle
		p_{Y}(y) = \int_{- \infty}^{\infty} p_{XY}(x,y) dx
	\end{equation}
	\subsection{Bedingte Verteilungsdichtefunktion}
	\begin{equation} \textstyle
		p_{X}(x|Y=y) := \frac{p_{XY}(x,y)}{p_{Y}(y)}
	\end{equation}
	\begin{equation} \textstyle
		p_{Y}(y|X=x) := \frac{p_{XY}(x,y)}{p_{X}(x)}
	\end{equation}
	\textbf{Statistische Unabhängigkeit}
	\begin{equation} \textstyle
		p_{XY}(x,y) = p_{X}(x) \cdot p_{Y}(y)
	\end{equation}
	\subsection{Diskete Zufallsvariablen}
	\begin{equation} \textstyle
		F_{X}(x) = \sum_{k=1}^{N} P_{k}\sigma(x-x_{k}), \hspace{0.5cm}\text{wobei } \sigma(x)  \text{ Sprungfuntion}
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \frac{dF_{X}(x)}{dx}
	\end{equation}
	\begin{equation} \textstyle
		p_{X}(x) = \sum_{k=1}^{N} P_{k}\delta(x-x_{k})
	\end{equation}
	\begin{equation} \textstyle
		P(a < X \leq b) = F_{X}(b) - F_{X}(a) = \int_{a}^{b} p_{X}(x) dx
	\end{equation}
	\textbf{Bernoulli-Experiment}
	\begin{equation} \textstyle
		P_{i} = P(\text{genau i-faches Auftreten}) = B(n,p,i) = \binom{n}{i} \cdot p^{i} \cdot (1-p)^{n-i}
	\end{equation}
	Bei kleinen p-Warten und großen N kann Poission-Verteilung verwendet werden
	\begin{equation} \textstyle
		P_{i} \approx \frac{(np)^{i}}{i!} e^{-np}
	\end{equation}
	\begin{equation} \textstyle
		P(\text{bis zu i-faches Auftreten}) = \sum_{j=0}^{i} B(n,p,j)
	\end{equation}
	\begin{equation} \textstyle
		P(\text{mehr als i-faches Auftreten}) = 1 - \sum_{j=0}^{i} B(n,p,j)
	\end{equation}
	\begin{equation} \textstyle
		\mu_{X} = np
	\end{equation}
	\begin{equation} \textstyle
		\sigma_{x}^{2} = np(1-p)
	\end{equation}
	\begin{equation} \textstyle
		P(A) = \sum_{k=1}^{N} P(A|B_{k}) P(B_{k}) 
	\end{equation}
	\textbf{FRAGE IM FORUM}
	\begin{equation} \textstyle
		P(A) = \sum_{k=1}^{N} P(A,B=B_{k})
	\end{equation}
	\subsection{Transformation}
	\begin{equation} \textstyle
		p_{Y}(y) = \frac{p_{X}(x)}{|g'(x)|}\Big|_{x=g^{-1}(y)}
	\end{equation}
	\begin{equation} \textstyle
		P(Y \leq y_{0}) = P(X \leq x_{0})
	\end{equation}
	\begin{equation} \textstyle
		\int_{- \infty}^{y_{0}} p_{y}(y) dy = \int_{- \infty}^{x_{0}=g^{-1}(y_{0})} p_{x}(x) dx
	\end{equation}
	\begin{equation} \textstyle
		p_{Y} = p_{X} \frac{d x_{0}}{d y_{0}} = \frac{p_{X}}{\frac{d y_{0}}{d x_{0}}}
	\end{equation}
	\textbf{Lineare Transformation $Y = aX + b$}
	\begin{equation} \textstyle
			p_{Y} = p_{X}\left( \frac{y-b}{a} \right) \frac{1}{|a|}
	\end{equation}
	\subsection{Summe von Zufallsvariablen}
	\textbf{$Z = X+Y$, wobei X und Y statistisch Unabhängig sein müssen:}
	\begin{equation} \textstyle
		p_{Z} = \int_{- \infty}^{\infty} p_{X}(\alpha)p_{Y}(z-\alpha) d \alpha = p_{X}(z) \ast p_{Y}(z)
	\end{equation}
	\textbf{Beweis}
	Aus $Z = X+Y$ wird $Y = Z-X$, und damit ist das Ereignis $(Z \leq a)$ identisch mit dem Verbundereignis ($Y \leq a - X$ und $- \infty \leq X \leq \infty )$. Für die Verteilungsfunktion $P(Z \leq a)$ gilt:
	\begin{equation} \textstyle
	P(Z \leq a) = P(X \leq \infty, Y \leq a -X) = F_{Z}(a) = \int_{x = - \infty}^{\infty} p_{XY}(x,a-x) dx
	\end{equation}
	Für die VDF folgt
	\begin{equation} \textstyle
	p_{z}(a) = \frac{F_{z}(a)}{da} = \int_{- \infty}^{\infty} p_{XY}(x,a-x) dx
	\end{equation}
	, die bei statistischer Unabhängigkeit zu
	\begin{equation} \textstyle
	p_{z}(a) = \int_{X} p_{X}(x)p_{Y}(a-x)dx = p_{X}(a) \ast p_{Y}(a)
	\end{equation}
	\subsection{Erwartungswerte}
	Der Erwartungswert E[(x)] einer transformierten Zufallsvariablen Y = g(X) ist definiert als
	\begin{equation} \textstyle
		E[g(x)] = \int_{- \infty}^{\infty} g(x) p_{X}(x) dx
	\end{equation}
	\textbf{Mittelwert}
	\begin{equation} \textstyle
		\mu_{x} = E[X] = \int_{- \infty}^{\infty} x p_{X}(x) dx
	\end{equation}
	\textbf{Quadratischer Mittelwert (Leistung)}
	\begin{equation} \textstyle
		P_{X} = E[X^{2}] = \int_{- \infty}^{\infty} x^{2} p_{X}(x) dx 
	\end{equation}
	\textbf{Varianz}
	\begin{equation} \textstyle
		\sigma_{X}^{2} = E[(X-\mu_{X})^{2}] = \int_{- \infty}^{\infty} (x-\mu_{X})^{2} p_{X}(x) dx
	\end{equation}
	Aus dieser Gleichung folgt auch
	\begin{equation} \textstyle
		\sigma_{X}^{2} = P_{X} - \mu_{X}^{2}
	\end{equation}
	\subsection{Erwartungswerte für wertdiskrete Zufallsvariablen}
	\begin{equation} \textstyle
		p_{X} = \sum_{k=1}^{N} P_{k} \delta(x-x_{k})
	\end{equation}
	\textbf{Mittelwert}
	\begin{equation} \textstyle
		\mu_{X} = E[X] = \sum_{k=1}^{N} P_{k} x_{k}
	\end{equation}
	\textbf{Quadratischer Mittelwert (Leistung)}
	\begin{equation} \textstyle
		\mu_{X} = E[X^{2}] = \sum_{k=1}^{N} P_{k} x_{k}^{2}
	\end{equation}
	\textbf{Varianz}
	\begin{equation} \textstyle
		\sigma_{X}^{2} = E[(X-\mu_{X})^{2}] = \sum_{k=1}^{N} P_{k} (x_{k}-\mu_{X})^{2}
	\end{equation}
	\subsection{Schätsgrößen für Erwartungswerte}
	\begin{equation} \textstyle
		m_{X} = \frac{1}{N} \sum_{n = 0}^{N - 1}x(n)
	\end{equation}
	\begin{equation} \textstyle
		E[M_{X}] = \frac{1}{M} \sum_{j = 1}^{M} \mu_{X} = \frac{1}{M}M \cdot \mu_{X} = \mu_{X}
	\end{equation}
	\begin{equation} \textstyle
		\lim_{N \rightarrow \infty} E[(M_{X} - \mu_{X})^{2}] = 0
	\end{equation}
	\begin{equation} \textstyle
		\sigma^{2}(m_{X}) = \frac{\sigma_{X}^{2}}{M}
	\end{equation}
	\textbf{Varianz}
	\begin{equation} \textstyle
		s_{X}^{2} = \frac{1}{N - 1} \sum_{n = 0}^{N - 1} [x(n) - m_{X}]^{2}
	\end{equation}
	\begin{equation} \textstyle
		E[S_{X}^{2}] = \sigma_{X}^{2} \hspace{0.5cm} ; \hspace{0.5cm} \sigma(S_{X}^{2}) = \frac{2\sigma_{X}^{2}}{N - 1}
	\end{equation}
	\subsection{Schätsgrößen für Erwartungswerte}
	\begin{equation} \textstyle
	E[Z] = \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} g(x,y) p_{XY}(x,y) dx dy
	\end{equation}
	Mit $Z = X+Y$, $Z = (X+Y)^{2}$, $Z = XY$, ergeben sich die folgenden Ergebnisse:
	\textbf{Addition von Zufallsvariablen}
	\begin{equation} \textstyle
		E[X+Y] = E[X] + E[Y]
	\end{equation}
	\textbf{Multiplikation von Zufallsvariablen}
	\begin{equation} \textstyle
		E[XY] = E[X] \cdot E[Y], \hspace{0.5cm} \text{ bei statist. Unabhängigkeit}
	\end{equation}
	\textbf{Multiplikation von Zufallsvariablen}
	\begin{equation} \textstyle
		E[(X+Y)^{2}] = P_{Z} = P_{X} + P_{Y} + 2 E[XY]
	\end{equation}
	\begin{equation} \textstyle
		\sigma_{Z}^{2} = \sigma_{X}^{2} + \sigma_{Y}^{2} \text{ bei statis. Unabhängigkeit}
	\end{equation}
	\textbf{Kovarianz}
	\begin{equation} \textstyle
		C_{XY} = E[(X - \mu_{X}) (Y - \mu_{Y})]
	\end{equation}
	\textbf{Kreuzkorrelationskoeffizient}
	\begin{equation} \textstyle
		p_{XY} = \frac{C_{XY}}{\sigma_{X} \sigma_{Y}}
	\end{equation}
	\textbf{Keine Korrelation}
	\begin{equation} \textstyle
		C_{XY} = 0
	\end{equation}
	\textbf{Orthogonak zueinander}
	\begin{equation} \textstyle
		E[XY] = 0
	\end{equation}
	Unkorreliertheit ist eine sehr viel schwächere Forderung als statistische Unabhängigkeit, da bei der letztern
	\begin{equation} \textstyle
		p_{XY}(x,y) = p_{X}(x) \cdot p_{Y}(y)
	\end{equation}
	f+r jedes $x$ und $y$ gelten muss, während $p_{XY}$ bei Unkorreliertheit unter einem Integral steht.
	\section{Stochastische Prozesse}
	\subsection{Eigenschaften}
	\begin{itemize}
		\item Stationär: TODO
		\item Ergodizität: TODO
	\end{itemize}
	\subsection{Ordnungen}
	\textbf{Verteilung erster Ordnung:} Für einen beliebigen Zeitpunkt $i$ und einen vorgegebenen Zahlenwert $\alpha$ beschreibt die Verteilungsfunktion die Verteilung der Amplitudenwerte im Ensemble:
	\begin{equation} \textstyle
		F_{X}(\alpha;i) = P(X(i) \leq \alpha)
	\end{equation}
	Für unterschiedlichste Zeitpunkte $i$ können die Verteilungsfunktion verschieden sein.\\
	\textbf{Verteilung zweiter Ordnung:} Für zwei beliebige Zeitpunkte $i$ und $j$ und zwei vorgegebene Zahlenwerten $\alpha$ und $\beta$ beschreibt die Verbundverteilungsfunktion den Zusammenhang zwischen den zu diesen Zeitpunkten auftretenden Amplitudenwerten:
	\begin{equation} \textstyle
	F_{X}(\alpha,\beta;i,j) = P(X(i) \leq \alpha; X(j) \leq \beta)
	\end{equation}
	\textbf{Verteilung dritter Ordnung:} Für drei beliebige Zeitpunkte $i$, $j$ und $k$ und drei vorgegebene Zahlenwerten $\alpha$, $\beta$ und $\gamma$ beschreibt die Verbundverteilungsfunktion den Zusammenhang zwischen den zu diesen Zeitpunkten auftretenden Amplitudenwerten:
	\begin{equation} \textstyle
	F_{X}(\alpha,\beta,\gamma;i,j,k) = P(X(i) \leq \alpha; X(j) \leq \beta; X(k) \leq \gamma)
	\end{equation}
	\textbf{Verallgemeinerung:} Ist die Beschreibung für die Verteilungsfunktionen beliebiger Ordnung bekannt, so ist der stochastische Prozess $\{ X(n) \}$ eindeutig beschrieben. Anstatt der Verbundverteilungsfunktionen können auch die Verbundverteilungsdichtefunktionen
	\begin{equation} \textstyle
		p_{X}(\alpha,\beta,\gamma,...;i,j,k,...)
	\end{equation}
	verwendet werden. Durch sie können die Wahrscheinlichkeiten
	\begin{equation} \textstyle
		p_{X}(\alpha,\beta,\gamma,...;i,j,k,...) d\alpha d\beta d\gamma
	\end{equation}
	angegeben werden, also die Wahrscheinlichkeiten, dass die Musterfolgen im Zeitpunkt $i$ Werte zwischen $\alpha$ und $\alpha + \delta \alpha$, im Zeitpunkt $j$ Werte zwischen $\beta$ und $\beta + \delta \beta$, im Zeitpunkt $k$ Werte zwischen $\gamma$ und $\gamma + \delta \gamma$ annehmen, etc.
	\subsection{Erwartungswerte eines stochatischen Prozesses}
	\textbf{Arithmetischer Mittelwert}
	\begin{equation} \textstyle
		\mu_{X}(n) := E[X(n)]
	\end{equation}
	Ist der Prozess stationär, so ist der Mittelwert unabhängig von dem absoluten Zeitpunkt n:
	\begin{equation} \textstyle
		\mu_{X} := E[X(n)]
	\end{equation}
	\textbf{Autokorrelationsfolge}
	\begin{equation} \textstyle
		R_{XX}(i,j) := E[X(i)X(j)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{X}(\alpha,\beta;i,j)d\alpha d\beta
	\end{equation}
	Ist der Mittlwert nur von der Zeitdifferent $k = j - i$ abhängig (also nicht von der absoluten Zeit $i$), so
	\begin{equation} \textstyle
		R_{XX}(n,n+k) := R_{XX}(k) := E[X(n)X(n+k)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{X}(\alpha,\beta;k)d\alpha d\beta
	\end{equation}
	\textbf{Autokovarianzfolge}
	\begin{equation} \textstyle
		C_{XX}(k) = E[(X(n) - \mu_{X}) ( X(n+k) - \mu_{X} )]
	\end{equation}
	Beschreibt die AKF des vom arithmetischen Mittelwert befreiten stochastischen Prozess $\{ X(n) \}$. Für mittelwertfreie Prozesse sind $R_{XX}(k)$ und $C_{XX}(k)$ identisch.\\
	\textbf{AKF Eigenschaften}
	\begin{equation} \textstyle
		R_{XX}(k) = R_{XX}(-k)
	\end{equation}
	\begin{equation} \textstyle
		R_{XX}(0) = P_{X}
	\end{equation}
	\begin{equation} \textstyle
		- R_{XX}(0) \leq  R_{XX}(k) \leq R_{XX}(0)
	\end{equation}
	Ist der Prozess nicht mittelwertfrei, so ist
	\begin{equation} \textstyle
		R_{XX}(0) = P_{X} = \sigma_{X}^{2} + \mu_{X}^{2}
	\end{equation}
	\begin{equation} \textstyle
		\lim R_{XX}(k) = 0
	\end{equation}
	\begin{equation} \textstyle
		p_{XX}(k) = \frac{R_{XX}(k)}{R_{XX}(0)}
	\end{equation}
	\begin{equation} \textstyle
	-1 \leq p_{XX}(k) \leq 1
	\end{equation}
	Kurzschreibweise
	\begin{equation} \textstyle
		p_{XX}(k) = p(k)
	\end{equation}
	\textbf{Autokorrelationsmatrix $R_{XX}$}
	\begin{equation} \textstyle
		\mathbf{R_{XX}} =
		 \begin{bmatrix}

		R_{XX}(0) &
 R_{XX}(1) & R_{XX}(2) & ... & R_{XX}(N-1)\\

		R_{XX}(1) & R_{XX}(0) & R_{XX}(1) & ... & R_{XX}(N-2)\\
		.. & ... & ... & ... & ...\\
		R_{XX}(N-1) & R_{XX}(N-2) & R_{XX}(1) & ... & R_{XX}(0)\\
		\end{bmatrix}
	\end{equation}
	\begin{equation} \textstyle
		|R_{XX}(k)| \geq 0
	\end{equation}
	\subsection{Kreuzkorrelationsfolgen}
	\begin{equation} \textstyle
		R_{XY}(i,j) := E[X(i)Y(j)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{XY}(\alpha,\beta;i,j)d\alpha d\beta
	\end{equation}
	Nur von der Zeitdifferenz abhängig...
	\begin{equation} \textstyle
		R_{XY}(n,n+k) := R_{XX}(k) := E[X(n)Y(n+k)] := \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \beta p_{XY}(\alpha,\beta;k)d\alpha d\beta
	\end{equation}
	\textbf{Eigenschaften KKF}
	\begin{equation} \textstyle
		R_{XY}(k) = R_{YX}(-k)
	\end{equation}
	\begin{equation} \textstyle
		R_{XY}^{2}(k) \leq R_{XX}(0) R_{YY})(0)
	\end{equation}
	\textbf{Unkorreliert}
	\begin{equation} \textstyle
		R_{XY}(k) = \mu_{X} \cdot \mu_{Y}, \hspace{0.5cm} \forall k
	\end{equation}
	\textbf{Statistische Unabhängigkeit}
	\begin{equation} \textstyle
		p_{XY}(\alpha,\beta;k) = p_{X}(\alpha;k)  \cdot p_{Y}(\beta;k)
	\end{equation}
	\textbf{Orthogonalität}
	\begin{equation} \textstyle
		R_{YX}(k) := 0; \hspace{0.5cm} \forall k
	\end{equation}
	\textbf{Addition von Zufallsvariable $Z(n) = X(n) + Y(n)$}
	\begin{equation} \textstyle
		R_{ZZ}(k) = R_{XX}(k) + R_{YY}(k) + R_{XY}(k) + R_{YX}(k)
	\end{equation}
	Wenn sie \textbf{nicht} miteinander korreliert sind
	\begin{equation} \textstyle
		R_{ZZ}(k) = R_{XX}(k) + R_{YY}(k), \hspace{0.5cm} P_{Z} = P_{X} + P_{Y}
	\end{equation}
	\subsection{Leistungsdichtespektrum}
	\begin{equation} \textstyle
		S_{XX}(\Omega) = \sum_{k=- \infty}^{\infty} R_{XX}(k) e^{-j k \Omega}
	\end{equation}
	\begin{equation} \textstyle
		R_{XX}(k) = \frac{1}{2\pi} \int_{- \pi}^{\pi} e^{j k \Omega} d \Omega
	\end{equation}
	\textbf{Eigenschaften}
	\begin{equation} \textstyle
		S_{XX}(\Omega) = S_{XX}(- \Omega)
	\end{equation}
	\begin{equation} \textstyle
		S_{XX}(\Omega) = S_{XX}^{*}(\Omega)
	\end{equation}
	\begin{equation} \textstyle
		S_{XX}(\Omega) \geq 0
	\end{equation}
	\begin{equation} \textstyle
		P_{X} = R_{XX}(0) = \frac{1}{2\pi} \int_{- \pi}^{\pi} S_{XX}(\Omega) d\Omega
	\end{equation}
	\begin{equation} \textstyle
		P_{X}(\Omega_{1}, \Omega_{2}) =  \frac{1}{\pi} \int_{\Omega_{1}}^{\Omega_{2}} S_{XX}(\Omega) d\Omega
	\end{equation}
	\textbf{Weißes Rauschen}
	\begin{equation} \textstyle
		R_{WW}(k) = \sigma_{W}^{2} \delta(k)
	\end{equation}
	\begin{equation} \textstyle
		S_{WW}(\Omega) = \sigma_{W}^{2}
	\end{equation}
	\textbf{Kreuz-LDS}
	\begin{equation} \textstyle
		S_{XY}(j\Omega) := \sum_{- \infty}^{\infty} R_{XY} e^{-jk\Omega}
	\end{equation}
	\begin{equation} \textstyle
		R_{XY}(k) = \frac{1}{2 \pi} \int_{- \pi}^{\pi} S_{XY}(j\Omega) e^{jk\Omega} d\Omega
	\end{equation}
	\textbf{Eigenschaften Kreuz LDS}
	\begin{equation} \textstyle
		S_{XY}(j\Omega) = S_{YX}(-j\Omega)
	\end{equation}
	\textbf{FORUM NACHGUCKEN!!!}
	\begin{equation} \textstyle
		S_{XY}(j\Omega) = S_{YX}^{*}(j\Omega)
	\end{equation}
	\subsection{Maß spektraler Konstanz (MSK)}
	\begin{equation} \textstyle
		\gamma_{X}^{2} = \frac{e^{\frac{1}{2\pi}\int_{-\infty}^{\infty}\ln S_{XX}(\Omega)d\Omega}}{\sigma_{X}^{2}}
	\end{equation}
	Mit $N$ Stützstellen im Abstand $\Delta = \frac{2\pi}{N}$:
	\begin{equation} \textstyle
		\gamma_{X}^{2} = \frac{\left[ \prod_{k=1}^{N} S_{XX}(k \Delta \Omega) \right]^{\frac{1}{N}}}{\frac{1}{N}\sum_{k=1}^{N}S_{XX}(k \Delta \Omega)}
	\end{equation}
	Bei endlichem N ist das Maß spektraler Konstanz durch das Verhältnis von geometrischen und arithmetischem Mittel der LDS-Stützstellen gegeben:
	\begin{equation} \textstyle
		\gamma_{X}^{2} = \frac{\text{Geometrisches Mittel der LDS-Stützstellen}}{\text{Arithmetisches Mittel der LDS-Stützstellen}}
	\end{equation}
	daher
	\begin{equation} \textstyle
		0 \leq \gamma_{X}^{2} \leq 1
	\end{equation}
	\textbf{Schätzung der Autokorrelationsfolge}
	\begin{equation} \textstyle
		\hat{R}_{XX}(k) = \frac{1}{N} \sum_{n=0}^{N-k-1} x(n) x(n+k); \hspace{0.5cm} k \geq 0
	\end{equation}
	\begin{equation} \textstyle
		\hat{R}_{XY}(k) = \frac{1}{N} \sum_{n=0}^{N-k-1} x(n) y(n+k); \hspace{0.5cm} k \geq 0
	\end{equation}
	\textbf{Schätzung der Autokorrelationsfolge}
	\begin{equation} \textstyle
		\hat{S}_{XX}(k) = \frac{1}{N} |X(j\Omega)|^{2}
	\end{equation}
	\begin{equation} \textstyle
		X(j\Omega) = \sum_{n=0}^{N-1} x(n)e^{-jn\Omega}
	\end{equation}
	\section{Lineare Systeme}
	\begin{equation} \textstyle
		h(n) \rightarrow H(j\Omega) = \sum_{- \infty}^{\infty} h(n) e^{-jn\Omega}
	\end{equation}
	\begin{equation} \textstyle
		y(n) = x(n) \ast h(n) = \sum_{- \infty}^{\infty} h(k) x(n-k)
	\end{equation}
	\begin{equation} \textstyle
		Y(j\Omega) = X(j\Omega) H(j\Omega)
	\end{equation}
	\textbf{Arithmetischer Mittelwert}
	\begin{equation} \textstyle
		\mu_{Y} = \mu_{X} \sum_{\forall k} h(k) = \mu_{X} H(0)
	\end{equation}
	\textbf{AKF}
	\begin{equation} \textstyle
		R_{YY}(k) = R_{XX}(k) \ast h(-k) \ast h(k) = R_{XX}(k) \ast R_{HH}(k)
	\end{equation}
	\textbf{Filter-AKF}
	\begin{equation} \textstyle
		R_{HH}(k) = h(k) \ast h(-k) = \sum_{\forall j} h(j) h(k+j)
	\end{equation}
	Ist $ \{ X(n) \}$ weißes Rauschen, so kann die Filter-AKF $R_{HH}(k)$ aus der Ausgangs - AKF bestimmt werden
	\begin{equation} \textstyle
		R_{HH}(k) = \frac{R_{YY}(k)}{\sigma_{X}^{2}}
	\end{equation}
	\textbf{KKF}
	\begin{equation} \textstyle
		R_{XY}(k) = R_{XX}(k) \ast h(k)
	\end{equation}
	\begin{equation} \textstyle
		R_{YX}(k) = R_{XX}(k) \ast h(-k)
	\end{equation}
	\textbf{LDS}
	\begin{equation} \textstyle
		S_{YY}(j\Omega) = S_{XX}(\Omega)|H(j\Omega)|^{2}
	\end{equation}
	\begin{equation} \textstyle
		S_{XY}(j\Omega) = S_{XX}(\Omega) H(j\Omega)
	\end{equation}
	\begin{equation} \textstyle
		S_{YX}(j\Omega) = S_{XX}(\Omega) H^{*}(j\Omega)
	\end{equation}
	Messung der Kreuzkorrelation mit weißem Rauschen $R_{XX}(k) = \sigma_{X}^{2} \delta(k)$
	\begin{equation} \textstyle
		R_{XY}(j\Omega) = \sigma_{X}^{2}\delta(k) \ast h(k) = \sigma_{X}^{2}h(k)
	\end{equation}
	\textbf{Filterung mit nichtrekursivem Filter}
	\begin{equation} \textstyle
		h(n) = \sum_{k=0}^{N-1} a_{k}\delta(n-k)
	\end{equation}
	\textbf{Filterung mit nichtrekursivem Filter}
	\begin{equation} \textstyle
		y(n) = \sum_{k=0}^{N-1} a_{k} x(n-k)
	\end{equation}
	Somit
	\begin{equation} \textstyle
		R_{YY} = E[Y(n)Y(n+k')] = \sum_{\alpha=0}^{N-1}\sum_{\beta=0}^{N-1} a_{\alpha} a_{\beta} R_{XX}( k + \alpha - \beta)
	\end{equation}
	Vektor
	\begin{equation} \textstyle
		\mathbf{x^{T}} = \{ x(n) \}; n=0,1,...,N
	\end{equation}
	\begin{equation} \textstyle
	\mathbf{R}_{XX} = \{ R_{XX}(|k-l) \}; n=0,1,...,N-1
	\end{equation}
	\begin{equation} \textstyle
	 \begin{bmatrix}
		R_{XX}(0) &
		R_{XX}(1) & R_{XX}(2) & ... & R_{XX}(N-1)\\
		
		R_{XX}(1) & R_{XX}(0) & R_{XX}(1) & ... & R_{XX}(N-2)\\
		.. & ... & ... & ... & ...\\
		R_{XX}(N-1) & R_{XX}(N-2) & R_{XX}(1) & ... & R_{XX}(0)\\
	\end{bmatrix}
	\end{equation}
	\begin{equation} \textstyle
		\mathbf{R}_{XX} = E[\mathbf{X}\mathbf{X^{T}}]
	\end{equation}
	Nochmal überlegen ob Seite 56/57 mehr reingenommen werden soll...
	\subsection{Zweidimensionale Prozesse}
	\textbf{Fouriertransformation}
	\begin{equation} \textstyle
		H(j\Omega_{h},j\Omega_{n}) := \sum_{m=0}^{N-1} \sum_{n=0}^{N-1} h(m,n) e^{-j(m\Omega_{h}+n\Omega_{v})}
	\end{equation}
	\begin{equation} \textstyle
		X(m,n) = \frac{1}{4\pi^{2}} \int_{- \pi}^{\pi} H(j\Omega_{h},j\Omega_{n}) e^{j(m\Omega_{h}+m\Omega_{v})}d\Omega_{h} d\Omega_{v}
	\end{equation}
	Fourierkoeffizienten
	\begin{equation} \textstyle
		t(k,l) := \frac{1}{N} \sum_{m=0}^{N-1} \sum_{n=0}^{N-1} x(m,n)e^{-j2\pi(km+ln)/N}
	\end{equation}
	Synthesegleichung
	\begin{equation} \textstyle
		x(m,n) := \frac{1}{N} \sum_{k=0}^{N-1} \sum_{l=0}^{N-1} t(k,l)e^{j2\pi(km+ln)/N}
	\Delta \Omega_{h} = \frac{2 \pi}{N}; \Delta \Omega_{v} = \frac{2 \pi}{N}
	\end{equation}
	\begin{equation} \textstyle
		\Delta \Omega_{h} = \frac{2 \pi}{N}; \Delta \Omega_{v} = \frac{2 \pi}{N}
	\end{equation}
	...
	\subsection{Zeitkontinuierliche stochastische Prozesse}
	\textbf{AKF}
	\begin{equation} \textstyle
		R_{XX}(\tau) = E[X(t)X(t+\tau)] = \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \cdot \beta \cdot p_{X}(\alpha,\beta;\tau) d\alpha d\beta
	\end{equation}
	\textbf{KKF}
	\begin{equation} \textstyle
		R_{XY}(\tau) = E[X(t)Y(t+\tau)] = \int_{- \infty}^{\infty} \int_{- \infty}^{\infty} \alpha \cdot \beta \cdot p_{XY}(\alpha,\beta;\tau) d\alpha d\beta
	\end{equation}
	\textbf{Wiener-Kitching-Beziehung}
	\begin{equation} \textstyle
		S_{XX}(\omega) = \int_{- \infty}^{\infty} R_{XX}(\tau) e^{-j\omega \tau} d\tau
	\end{equation}
	\begin{equation} \textstyle
		R_{XX}(\tau) = \frac{1}{2\pi} \int_{- \infty}^{\infty} S_{XX}(\omega) e^{j\omega \tau} d\omega
	\end{equation}
	\textbf{Kreuz-LDS}
	\begin{equation} \textstyle
		S_{XY}(\omega) = \int_{- \infty}^{\infty} R_{XY}(\tau) e^{-j\omega \tau} d\tau
	\end{equation}
	\begin{equation} \textstyle
		R_{XY}(\tau) = \frac{1}{2\pi} \int_{- \infty}^{\infty} S_{XY}(\omega) e^{j\omega \tau} d\omega
	\end{equation}
	\textbf{Lieare Systeme}\\
	\textbf{AKF}
	\begin{equation} \textstyle
		R_{YY}(\tau) = h(\tau) \ast h(- \tau) \ast R_{XX}(\tau)
	\end{equation}
	\textbf{KKF}
	\begin{equation} \textstyle
		R_{XY}(\tau) = R_{XX}(\tau) \ast h(\tau)
	\end{equation}
	\begin{equation} \textstyle
		R_{YX}(\tau) = R_{XX}(\tau) \ast h(- \tau)
	\end{equation}
	\textbf{LDS}
	\begin{equation} \textstyle
		S_{YY}(\omega) = S_{XX}(\omega)|H(j\omega)|^{2}
	\end{equation}
	\begin{equation} \textstyle
		S_{XY}(j \omega) = S_{XX}(\omega)H(j\omega)
	\end{equation}
	\begin{equation} \textstyle
		S_{YX}(j \omega) = S_{XX}(\omega)H^{*}(j\omega)
	\end{equation}
	Vielleicht weißes rauschen Seite 67\\
	\textbf{Noch Seite 69/70}
	\section{Störreduktion}
	\subsection{Schätzung einer Zufallsvariablen}
	\textbf{Schätzung einer Zufallsvariable durch eine Konstante}
	Ziel ist es die Varianz $\sigma_{R}^{2}$ des Fehlers $R = X - a$ zu minieren:
	\begin{equation} \textstyle
			\sigma_{R}^{2} = E[(X-a)^{2}]
	\end{equation}
	\textbf{Schätzung einer Zufallsvariable durch eine andere Zufallsvariable Y}
	\begin{equation} \textstyle
		\sigma_{R}^{2} = E[(X-\hat{X})^{2}] = E[(X-aY)^{2}]
	\end{equation}
	Die Ableitung nach a
	\begin{equation} \textstyle
		a_{opt} = \frac{E[XY]}{\sigma_{Y}^{2}} = \frac{R_{XY}(0)}{\sigma_{Y}^{2}} = \frac{\sigma_{X}}{\sigma_{Y}} \cdot p_{XY}
	\end{equation}
	\textbf{Orthogonalitätsprinzip}\\
	Y muss orthogonal zu dem Schätzfehler $R_{opt} = X - \hat{X}$ sein
	\begin{equation} \textstyle
		E[Y(X-a_{opt}Y)] = E[Y(X-\hat{X}_{opt})] = 0
	\end{equation}
	\subsection{Rauschreduktion durch zeitliches Überlagern}
	S. 76
	\subsection{Optimale Prä-Deemphase-Systeme}
	\textbf{TODO Bild einfügen}\\
	Bei einer verzerrunfsfreien Übertragung muss gelten:
	\begin{equation} \textstyle
		A_{S}(\Omega) \cdot A_{E}(\Omega) = 1 = const
	\end{equation}
	Lineare Phase
	\begin{equation} \textstyle
		\varphi_{S}(\Omega) + \varphi_{E}(\Omega) = - \Omega t_{d}
	\end{equation}
	\begin{equation} \textstyle
		|H_{E}(j\Omega)|^{2} = |H_{S}(j\Omega)|^{-2}
	\end{equation}
	\begin{equation} \textstyle
		P_{aus} = \frac{1}{2 \pi} \int_{- \pi}^{\pi} S_{XX}(\Omega) d\Omega
	\end{equation}
	\textbf{Störleistung}
	\begin{equation} \textstyle
		P_{N,aus} = \frac{1}{2 \pi} \int_{- \pi}^{\pi} S_{NN}(\Omega) |H_{E}(j\Omega)|^{2} d\Omega
	\end{equation}
	\textbf{SNR}
	\begin{equation} \textstyle
		SNR = \frac{P_{ein}}{P_{N,aus}} = \frac{\frac{1}{2 \pi} \int_{- \pi}^{\pi} S_{XX}(\Omega) d\Omega}{\frac{1}{2 \pi} \int_{- \pi}^{\pi} S_{NN}(\Omega) |H_{E}(j\Omega)|^{2} d\Omega}
	\end{equation}
	\begin{equation} \textstyle
	P_{Sender} = \frac{1}{2 \pi} \int_{- \pi}^{\pi} S_{XX}(\Omega) |H_{S}(j\Omega)|^{2} d\Omega
	\end{equation}
	\begin{equation} \textstyle
	V_{A} = 2\pi \frac{P_{Sender}  \int_{- \pi}^{\pi} S_{XX}(\Omega) d\Omega}{\int_{- \pi}^{\pi} S_{XX}(\Omega) |H_{S}(j\Omega)|^{2} d\Omega \cdot \int_{- \pi}^{\pi} S_{XX}(\Omega) |H_{E}(j\Omega)|^{2} d\Omega}
	\end{equation}
	\textbf{Cauchy-Schwarze Ungleichung}
	\begin{equation} \textstyle
		\int a^{2}(t) dt \cdot \int b^{2}(t) dt \geq \left| \int a(t) b(t) \right|^{2}
	\end{equation}
	... S. 81
	\begin{equation} \textstyle
		|H_{S}(j\Omega)|^{2} = \sqrt{\frac{S_{NN}(\Omega)}{S_{XX}(\Omega)}}
	\end{equation}
	\begin{equation} \textstyle
		|H_{SE}(j\Omega)|^{2} = \sqrt{\frac{S_{XX}(\Omega)}{S_{NN}(\Omega)}}
	\end{equation}
	\begin{equation} \textstyle
		\max{\{SNR\}} = \frac{P_{Sender} P_{ein}}{\frac{1}{2 \pi} \left[ \int_{- \pi}^{\pi} \sqrt{S_{XX}(\Omega) S_{NN}(\Omega)} d\Omega \right]^{2}}
	\end{equation}
	\subsection{Wiener - Kologoroff - Filterung}
	\begin{itemize}
		\item Rauschbefreiung gestörter Nachrichtensignale
		\item Entzerrung von nachrichtensignalen
		\item Echobrefreiung von Nachrichtensignalen
		\item Vorhersage von Nachrichtensignalen (Prädiktion)
	\end{itemize}
	\textbf{TODO: ADD BILD!}
	Es muss der mittlerer quadratische Fehler (Leistung) minimiert werden
	\begin{equation} \textstyle
		\sigma_{r}^{2} = E[\{X(n) - Z(n)\}^{2}]
	\end{equation}
	\begin{equation} \textstyle
		z(n) = \sum_{k \in I} h_{k} y(n-k)
	\end{equation}
	\begin{equation} \textstyle
		z(n) = \mathbf{h^{T}y}
	\end{equation}
	\textbf{Indexmengen}
	\begin{itemize}
		\item $I=[0,N]$: Endliches kausales WK-Filter.
		\item $I=[0,\infty]$: Kausales WK-Filter.
		\item $I=[-M,N]$: Endliches WK-Filter mit Verzögerung.
		\item $I=[-\infty,\infty]$: Nichtkausales WK-Filter.
	\end{itemize}
	\subsection{Optimierung des WK-Filters (S. 86)}
	\begin{equation} \textstyle
		\sigma_{R}^{2} = E\left[ \left\{ X(n) - \sum_{k \in I} h_{k} Y(n-k) \right\}^{2} \right]
	\end{equation}
	\begin{equation} \textstyle
	E\left[ \left\{ X(n) - Z_{opt}(n) \right\} Y(n-k)  \right] = 0; k \in I
	\end{equation}
	\textbf{Wiener-Hopf-Bedingung}
	\begin{equation} \textstyle
		R_{YY}(k) = R_{YZ_{opt}}(k); k \in I
	\end{equation}
	\begin{equation} \textstyle
		E\left[ \left\{ X(n) - Z_{opt}(n) \right\} Z_{opt}(n) \right] = 0
	\end{equation}
	\textbf{Wiener-Hopf-Gleichung}
	\begin{equation} \textstyle
		R_{YX}(j) = \sum_{k \in I} h_{k,opt} R_{YY}(j-k); j \in I
	\end{equation}
	... S. 87
	\subsection{Ergebnisse für verschiedene Filterklassen}
	\textbf{Endliches Kausales Filter}
	\begin{equation} \textstyle
		R_{YX}(j) = \sum_{k=0}^{N} h_{k,opt} R_{YY}(j-k)
	\end{equation}
	\begin{equation} \textstyle
		\begin{bmatrix}
		R_{YY}(0) & R_{YY}(1) & ... & R_{YY}(N)\\
		R_{YY}(1) & R_{YY}(0) & ... & R_{YY}(N-1)\\
		.. & ... & ... & ... & ...\\
		R_{YY}(N) & R_{YY}(N-1) & ... & R_{YY}(0)\\
		\end{bmatrix}
		\begin{bmatrix}
		h_{0,opt} \\
		h_{1,opt} \\
		... \\
		h_{N,opt} \\
		\end{bmatrix}
		=
		\begin{bmatrix}
		R_{YX}(0) \\
		R_{YX}(1) \\
		... \\
		R_{YX}(N) \\
		\end{bmatrix}
	\end{equation}
	\textbf{Endliches WK-Filter mit Verzögerung $I=[-M,N]$}
	\begin{equation} \textstyle
		R_{YX}(j) = \sum_{k=-M}^{N-M} h_{k,opt} R_{YY}(j-k)
	\end{equation}
	\textbf{Nichtkausales Filter $I=[-\infty,\infty]$}
	\begin{equation} \textstyle
		R_{YX}(j) = \sum_{k=-\infty}^{\infty} h_{k,opt} R_{YY}(j-k) = h_{j,opt} \ast R_{YY}(j)
	\end{equation}
	\textbf{Übertragunsfunktion Nichtkausales Filter}
	\begin{equation} \textstyle
		H_{opt}(j \Omega) = \frac{S_{YX}(j\Omega)}{S_{YY}(\Omega)}
	\end{equation}
	\textbf{Minimale Fehlervarianz Nichtkausales Filter}
	\begin{equation} \textstyle
		\min{\{\sigma_{R}^{2}\}} = \sigma_{X}^{2} - \frac{1}{2 \pi} \int_{-\pi}^{\pi} \frac{|S_{YX}|^{2}}{S_{YY}(\Omega)} d\Omega
	\end{equation}
	\textbf{Additive Rauschstörung: $S_{XX}(\Omega) = S_{XX}(\Omega) + S_{NN}(\Omega)$}
	\begin{equation} \textstyle
		H_{opt}(j\Omega) = \frac{S_{XX}(\Omega)}{S_{XX}(\Omega)+S_{NN}(\Omega)}
	\end{equation}
	\section{Lineare Prädiktion}
	\subsection{Wiener-Hopf-Gleichung}
	\begin{equation} \textstyle
		z(n) = \sum_{k=1}^{N} h_{k} x(n-k)
	\end{equation}
	\textbf{Prädikator hat Impulsantwort}
	\begin{equation} \textstyle
		h(n) = \sum_{k=1}^{N} h_{k} \delta(n-k)
	\end{equation}
	\textbf{Übertragungsfunktion}
	\begin{equation} \textstyle
		H(z) = \sum_{k=1}^{N} h_{k} z^{-k}
	\end{equation}
	\textbf{Varianz des Prdikationsfehlersignals $d(n) = x(n) - z(n)$}
	\begin{equation} \textstyle
		\sigma_{D} = E[ \{ X(n) - Z(n) \}^{2} ]
	\end{equation}
	\textbf{Orthogonalitätsprinzip}
	\begin{equation} \textstyle
		E[ \{ X(n) - Z_{opt}(n) \}X(n-k) ] = 0; \hspace{0.5cm} k \in (1,N)
	\end{equation}
	\textbf{Wiener-Hopf-Gleichung für die Lineare Prädiktion}
	\begin{equation} \textstyle
		R_{XX}(j) = \sum_{k=1}^{N} h_{k,opt} R_{XX}(j-k); \hspace{0.5cm} j=1,2,...,N
	\end{equation}
	\begin{equation} \textstyle
		\begin{bmatrix}
		R_{XX}(0) & R_{XX}(1) & ... & R_{XX}(N-1)\\
		R_{XX}(1) & R_{XX}(0) & ... & R_{XX}(N)\\
		.. & ... & ... & ... & ...\\
		R_{XX}(N-1) & R_{XX}(N-2) & ... & R_{XX}(0)\\
		\end{bmatrix}
		\begin{bmatrix}
		h_{1,opt} \\
		h_{2,opt} \\
		... \\
		h_{N,opt} \\
		\end{bmatrix}
		=
		\begin{bmatrix}
		R_{XX}(1) \\
		R_{XX}(2) \\
		... \\
		R_{XX}(N) \\
		\end{bmatrix}
	\end{equation}
	\textbf{Minimale Fehlervarianz}
	\begin{equation} \textstyle
		\min{\{ \sigma_{D}^{2}(\infty) \}} = \gamma_{X}^{2} \cdot \sigma_{X}^{2} = e^{\frac{1}{2 \pi} \int \ln(S_{XX}(\Omega))d\Omega}
	\end{equation}
	\subsection{Prädiktionsgewinn}
	\begin{equation} \textstyle
		G_{P} = \frac{\sigma_{X}^{2}}{\sigma_{D}^{2}}
	\end{equation}
	\begin{equation} \textstyle
		\max{\{G_{P}\}} = G_{P}(\infty) = \gamma_{X}^{-2}
	\end{equation}
	\section{Bayes}
	\subsection{Satz von Bayes}
	\begin{equation} \textstyle
		P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
	\end{equation}
	\begin{equation} \textstyle
		P(\omega_{j} | x) = \frac{p(x|\omega_{j}) \cdot P(\omega_{j})}{p(x)}
	\end{equation}
	\textbf{posterior:} the probability of the state of nature being $\omega_{j}$ given that the feature value $x$ has been measured\\
	\textbf{likelihood:} $p(x|\omega_{j})$ is the likelihood of $\omega_{j}$ with respect to $x$. The category $\omega_{j}$, for which $p(x|\omega_{j})$ is large is more "likely" to be true category\\
	\textbf{prior:} scale factor\\
	\textbf{evidence:} scale factor\\
	\begin{equation} \textstyle
		\text{posterior} = \frac{\text{likelihood} \cdot \text{prior}}{\text{evidence}}
	\end{equation}	
	Ein Modell $M$ sollmit einem Datensatz $D$ untersucht werden. Die Ausgangsfragestellung ist, wie die Wahrscheinlichkeiten für die Modellparameter $M$ verteilt sind, sofern die Daten $D$ gegeben sind. Es soll also ein Ausdruck für $P(M|D)$ gefunden werden.\\
	\begin{equation} \textstyle
		P(M|D) = \frac{P(D|M)P(M)}{P(D)}
	\end{equation}
	\begin{itemize}
		\item $P(M)$: A-priori-Wahrscheinlichkeit: Also die Wahrscheinlichkeitsverteilung für $M$, ohne die Messdaten $D$
		\item $P(M|D)$: A-posteriori-Wahrscheinlichkeit: Also die Wahrscheinlichkeitsverteilung für $M$, mit den Messdaten $D$
		\item $P(D|M)$: Likelihood: Auch inverse Wahrscheinlichkeit oder "Plausabilität" genannt. Die Wahrscheinlichkeitverteilung für die Messdaten $D$, wenn der Modellparameter $M$ gegeben ist
	\end{itemize}
	\textbf{Durchschnittliche Risiko $R(\tau)$}\\
	Das Risiko einer Entscheidungsregel $\alpha(x)$ ist das Integral über das bedingte Risiko dieser Regel gegeben eines Messwertes
	\begin{equation} \textstyle
		R(\alpha(x)) = \oint R(\alpha(x)|x)  p_{X}(x) dx
	\end{equation}
	Das bedingte Risiko ist das mittlere bedingte Risiko je Aktion
	\begin{equation} \textstyle
		R(\alpha(x)|x) = \sum_{\forall i} R(\alpha_{i}|x)  P(\alpha_{i}|x) dx
	\end{equation}
	Das mittlere bedingte Risiko je Aktion wiederum ist das Mittel der Kosten unter dem Wissen des wahren state-of-nature
	\begin{equation} \textstyle
		R(\alpha_{i}|x)  = \sum_{\forall j} \lambda(\alpha_{i}|\omega_{j}) P(\omega_{j}|x)
	\end{equation}
	\textbf{Diskriminanzfunktion $g(x)$}
	\begin{equation} \textstyle
		g_{1}(x) = p(x|\omega = 1) \cdot P(\omega = 1)
	\end{equation}
	\begin{equation} \textstyle
		g_{2}(x) = p(x|\omega = 2) \cdot P(\omega = 2)
	\end{equation}
	\subsection{Optimale Schätzung}
	Kann mit Mittelwert der Dichtefunktion oder MAP bestimmt werden.
	\textbf{Mittelwert der Dichtefunktion}\\
	\begin{equation} \textstyle
		\hat{Y}_{\mu} = E[p(y|X_{1} = 1, X_{2} = x_{2}, ..., X_{k} = x_{k})]
	\end{equation}
	\textbf{Maximum A-posteriori (MAP)}
	Bei einer diskreten ZV $Y$ ist die MAP-Schätzung $\underline{x} \rightarrow \hat{Y}$ also identisch mit der Bayesschen Entscheidung!
	\begin{equation} \textstyle
		\hat{Y}_{MAP} = \arg \max_{y_{i}}\{ P({y_{1}}|\underline{x}), P({y_{2}}|\underline{x}), ..., P(y_{M}|\underline{x})) \}
	\end{equation}
	\textbf{Orthogonalitätsprinzip}
	Beim Orthogonalitätsprinzip, wird die Fehlervarianz $\sigma_{R}^{2} = E[(Y-\hat{Y})^{2}]$ zum Minimum $\sigma_{R}^{2} = \min\{ \sigma_{R}^{2} \}$\\
	Bei optimaler Wahl der Parameter $a_{k} = a_{k,opt}$ für alle $k$ ist der Fehler $R$
orthogonal zu jeder der zur Schätzung herangezogenen ZV $X_{k}$
	\begin{equation} \textstyle
		E[R \cdot X_{j}] = 0 = E\left[ (Y - \sum_{k=1}^{K} a_{k,opt} X_{k}) X_{j} \right]
	\end{equation}
	\begin{equation} \textstyle
		E[Y \cdot X_{j}] = \sum_{k=1}^{K} a_{k,opt} E[X_{k}X_{j}], \hspace{0.5cm} \text{für } j=1,...,K
	\end{equation}
	\begin{equation} \textstyle
	\begin{bmatrix}
		E[Y \cdot X_{1}] \\
		E[Y \cdot X_{2}] \\
		... \\
		E[Y \cdot X_{k}] \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		E[X_{1}X_{1}] &E[X_{1}X_{2}] & ... & E[X_{1}X_{K}]\\
		E[X_{2}X_{1}] & E[X_{2}X_{2}] & ... & E[X_{2}X_{k}]\\
		.. & ... & ... & ... & ...\\
		E[X_{K}X_{1}] & E[X_{K}X_{2}] & ... & E[X_{K}X_{K}]\\
		\end{bmatrix}
		\begin{bmatrix}
		a_{1,opt} \\
		a_{2,opt} \\
		... \\
		a_{K,opt} \\
		\end{bmatrix}
	\end{equation}
	\begin{equation} \textstyle
		\underline{r}_{YX} = \underline{R}_{XX} \cdot a_{opt}
	\end{equation}
	\begin{equation} \textstyle
		 a_{opt} = \underline{R}_{XX}^{-1} \cdot \underline{r}_{YX}
	\end{equation}
	\textbf{Schätzfehler}
	\begin{equation} \textstyle
		\min{ \{ \sigma_{R}^{2} \} } = E[ ( Y - \sum_{k=1}^{K} a_{k,opt} \cdot X_{k} )^{2	}  ]
	\end{equation}
	\begin{equation} \textstyle
		\min{ \{ \sigma_{R}^{2} \} } = E[ (Y - \underline{a}_{opt}^{T} \cdot \underline{X})^{2} ]
	\end{equation}
	\begin{equation} \textstyle
		\min \{ \sigma_{R}^{2} \} = \sigma_{Y}^{2} - \underline{a}_{opt}^{T} \cdot \underline{r}_{YX}
	\end{equation}
	\begin{equation} \textstyle
		\min \{ \sigma_{R}^{2} \} = \sigma_{Y}^{2} - \underline{r}_{YX}^{T} \cdot \underline{R}_{XX}^{-1} \cdot \underline{r}_{YX}
	\end{equation}
	Es lässt sich der minimale Fehler bestimmen ohne die Schätzparameter zu bestimmen!
	
	
	\section{Matrix}
	\textbf{Symmetrische Matrix}
	\begin{equation} \textstyle
		M^{T} = M
	\end{equation}
	\textbf{Transponierung Multiplikation}
	\begin{equation} \textstyle
		(A \cdot B)^T = B^T \cdot A^T
	\end{equation}
	Inverse 2x2 Matrix
	\begin{equation} \textstyle
		\begin{bmatrix}
		a & b\\
		c & d\\
		\end{bmatrix}
		= 
		\frac{1}{ad-bc}
		\begin{bmatrix}
		d & -b\\
		-c & a\\
		\end{bmatrix}
	\end{equation}
\end{document}